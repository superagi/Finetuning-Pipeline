# DATA PARAMETERS
BETA: 0.1

# TRAINING PARAMETERS
MODEL_NAME: "SAGI-1/AGI_1_2_500k_LoRA"
LEARNING_RATE: 0.001
LR_SCHEDULER_TYPE: "cosine"
WARMUP_STEPS: 100
WEIGHT_DECAY: 0.01
OPTIMIZER_TYPE: "adamw_hf"

# BATCH_SIZE = PER_DEVICE_TRAIN_BATCH_SIZE*GRADIENT_ACCUMULATION_STEPS*NO_OF_GPUS
PER_DEVICE_TRAIN_BATCH_SIZE: 1
PER_DEVICE_EVAL_BATCH_SIZE: 1
GRADIENT_ACCUMULATION_STEPS: 1
GRADIENT_CHECKPOINTING: true

LORA_ALPHA: 16.0
LORA_DROPOUT: 0.05
LORA_R: 8

MAX_PROMPT_LENGTH: 2048
MAX_LENGTH: 2048
MAX_STEPS: 500
LOGGING_STEPS: 10
SAVE_STEPS: 100
EVAL_STEPS: 100
EVALUATION_STRATEGY: "steps"

OUTPUT_DIR: "./results"
LOG_FREQ: 1
LOAD_IN_4BIT: false
USE_BF16: true
SAVE_TOTAL_LIMIT: 2
REMOVE_UNUSED_COLUMNS: false
LOAD_BEST_MODEL_AT_END: true

# INSTRUMENTATION
SANITY_CHECK: false
REPORT_TO: "wandb"

# DEBUG ARGUMENT FOR DISTRIBUTED TRAINING
IGNORE_BIAS_BUFFERS: false

# DATASET
DATASET_NAME: "SAGI-1/ultrafeedback_binarized_dpo"

# HUGGINGFACE LOGIN
HUGGINGFACE_AUTH_TOKEN: "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"
